{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile, glob, random\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown\n",
    "from PIL import Image\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# LLM: openai and google_genai\n",
    "import openai\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# LLM: HuggingFace\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# langchain prompts, memory, chains...\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import Document, format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "# Document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    DirectoryLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredExcelLoader,\n",
    "    Docx2txtLoader,\n",
    ")\n",
    "\n",
    "# Text Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chroma: vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Contextual Compression\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter,LongContextReorder\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Cohere\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain_community.llms import Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Directories: where temp files and vectorstores will be saved\n",
    "\n",
    "TMP_DIR = Path(\"./data\").resolve().parent.joinpath(\"data\", \"tmp\")\n",
    "LOCAL_VECTOR_STORE_DIR = Path(\"./data\").resolve().parent.joinpath(\"data\", \"vector_stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO]: HUGGINGFACEHUB_API_TOKEN retrieved successfully.\n",
      "\n",
      "[INFO]: COHERE_API_KEY retrieved successfully.\n",
      "\n",
      "[INFO]: GOOGLE_API_KEY retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "def get_environment_variable(key):\n",
    "    if key in os.environ:\n",
    "        value = os.environ.get(key)\n",
    "        print(f\"\\n[INFO]: {key} retrieved successfully.\")\n",
    "    else:\n",
    "        print(f\"\\n[ERROR]: {key} is not found in your environment variables.\")\n",
    "        value = getpass(f\"Insert your {key}: \")\n",
    "    return value\n",
    "\n",
    "HF_key = get_environment_variable(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "cohere_api_key = get_environment_variable(\"COHERE_API_KEY\")\n",
    "google_api_key = get_environment_variable(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_document_loader(TMP_DIR):\n",
    "    \"\"\"\n",
    "    Load files from TMP_DIR (temporary directory) as documents. Files can be in txt, pdf, CSV or docx format.\n",
    "    https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    txt_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True\n",
    "    )\n",
    "    documents.extend(txt_loader.load())\n",
    "\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(), glob=\"**/*.pdf\", loader_cls=PyPDFLoader, show_progress=True\n",
    "    )\n",
    "    documents.extend(pdf_loader.load())\n",
    "\n",
    "    csv_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(), glob=\"**/*.csv\", loader_cls=CSVLoader, show_progress=True,\n",
    "        loader_kwargs={\"encoding\":\"utf8\"}\n",
    "    )\n",
    "    documents.extend(csv_loader.load())\n",
    "\n",
    "    doc_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(),\n",
    "        glob=\"**/*.docx\",\n",
    "        loader_cls=Docx2txtLoader,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    documents.extend(doc_loader.load())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of documents: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = langchain_document_loader(TMP_DIR)\n",
    "print(f\"\\nNumber of documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Names:\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n",
      "C:\\d drive\\Anurag\\MAchine Learning\\Projects\\Rag_langchain_chatbot\\data\\tmp\\2024-01-24-VirtualTryAll.pdf\n"
     ]
    }
   ],
   "source": [
    "document_names = [doc.metadata.get('source') for doc in documents] \n",
    "\n",
    "print(\"\\nDocument Names:\")\n",
    "for name in document_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Document[7]** \n",
       "\n",
       " **Page content** (first 1000 character):\n",
       "\n",
       "Table 1. Quantitative comparison between DTC variants and\n",
       "PBE best, which denotes a PBE variant using DINOv2 and percep-\n",
       "tual loss. CA denotes Cross-Attention.\n",
       "Method CLIP Score ( ↑) FID ( ↓)\n",
       "PBE best 85.43 6.65\n",
       "Ours addition 86.94 6.19\n",
       "Ours CA 88.01 5.68\n",
       "Ours FiLM 88.14 5.72\n",
       "integration of more computationally expensive Cross Atten-\n",
       "tion layers [43]. Results shown on Tab. 1 revealed that both\n",
       "FiLM and Cross Attention layers outperform direct addi-\n",
       "tion. Also, Cross Attention and FiLM yield comparable re-\n",
       "sults, and FiLM is cheaper to compute, therefore we chose\n",
       "to use FiLM in our final model.\n",
       "Figure 7. Failure cases with generating fine-grained text.\n",
       "4.4. Evaluation and Comparisons\n",
       "Comparison Against Paint by Example Variants. We\n",
       "implemented a series of enhancements to PBE and trained\n",
       "each variant on VITONHD-NoFace dataset. The results are\n",
       "presented in Table 2. As anticipated, using all CLIP patches\n",
       "surpasses the performance of using only the [CLS] token,\n",
       "which is limited to encoding  ...\n",
       "\n",
       "**Metadata:**\n",
       "\n",
       "{'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf', 'page': 7}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_document_id = random.choice(range(len(documents)))\n",
    "\n",
    "Markdown(f\"**Document[{random_document_id}]** \\n\\n **Page content** (first 1000 character):\\n\\n\" +\\\n",
    "         documents[random_document_id].page_content[0:1000] + \" ...\"  +\\\n",
    "         \"\\n\\n**Metadata:**\\n\\n\" + str(documents[random_document_id].metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks: 45\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],    \n",
    "    chunk_size = 1600,\n",
    "    chunk_overlap= 200\n",
    ")\n",
    "\n",
    "# Text splitting\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "print(f\"number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def tiktoken_tokens(documents,model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Use tiktoken (tokeniser for OpenAI models) to return a list of token lengths per document.\"\"\"    \n",
    "    encoding = tiktoken.encoding_for_model(model) # returns the encoding used by the model.\n",
    "    \n",
    "    tokens_length = [len(encoding.encode(documents[i].page_content)) for i in range(len(documents))]\n",
    "\n",
    "    return tokens_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens - Average : 291\n",
      "Number of tokens - 25% percentile : 165\n",
      "Number of tokens - 50% percentile : 350\n",
      "Number of tokens - 75% percentile : 373\n",
      "\n",
      "Max_tokens for gpt-3.5-turbo: 4096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks_length = tiktoken_tokens(chunks,model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(f\"Number of tokens - Average : {int(np.mean(chunks_length))}\")\n",
    "print(f\"Number of tokens - 25% percentile : {int(np.quantile(chunks_length,0.25))}\")\n",
    "print(f\"Number of tokens - 50% percentile : {int(np.quantile(chunks_length,0.5))}\")\n",
    "print(f\"Number of tokens - 75% percentile : {int(np.quantile(chunks_length,0.75))}\")\n",
    "print(\"\\nMax_tokens for gpt-3.5-turbo: 4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def select_embeddings_model(LLM_service=\"OpenAI\"):\n",
    "    \"\"\"Connect to the embeddings API endpoint by specifying the name of the embedding model.\"\"\"\n",
    "    if LLM_service == \"OpenAI\":\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model='text-embedding-ada-002',\n",
    "            api_key=openai_api_key)\n",
    "\n",
    "    if LLM_service == \"Google\":\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=google_api_key\n",
    "        )\n",
    "    \n",
    "         \n",
    "    return embeddings\n",
    "\n",
    "embeddings_google = select_embeddings_model(LLM_service=\"Google\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarty of sentences (0, 1): 0.904\n",
      "Similarty of sentences (0, 2): 0.679\n",
      "Similarty of sentences (1, 2): 0.687\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sentences = [\"I like pets.\",\n",
    "             \"Pets bring joy to our lives.\",\n",
    "             \"Langchain is a framework for developing applications powered by LLMs.\"]\n",
    "# 1. Calculate embedding vectors\n",
    "embedding_vectors = [embeddings_google.embed_query(sentence) for sentence in sentences]\n",
    "\n",
    "for combination in list(combinations(range(len(sentences)),2)):\n",
    "    # 2. Calculate similarity using dot product from numpy:\n",
    "    dot_prodduct = round(np.dot(embedding_vectors[combination[0]], embedding_vectors[combination[1]]),3)\n",
    "    print(f\"Similarty of sentences {combination}: {dot_prodduct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_store_google: 540 chunks.\n"
     ]
    }
   ],
   "source": [
    "vector_store_google = Chroma(\n",
    "    persist_directory = LOCAL_VECTOR_STORE_DIR.as_posix() + \"/Vit_All_Google_Embeddings\",\n",
    "    embedding_function=embeddings_google)\n",
    "print(\"vector_store_google:\",vector_store_google._collection.count(),\"chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_store_google: 585 chunks.\n"
     ]
    }
   ],
   "source": [
    "def create_vectorstore(embeddings, documents, vectorstore_name):\n",
    "    \"\"\"Create a Chroma vector database.\"\"\"\n",
    "    persist_directory = (LOCAL_VECTOR_STORE_DIR.as_posix() + \"/\" + vectorstore_name)\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return vector_store, vectorstore_name  # Return the original vectorstore_name\n",
    "\n",
    "create_vectorstores = True \n",
    "\n",
    "if create_vectorstores:\n",
    "    vector_store_google, _ = create_vectorstore(\n",
    "        embeddings=embeddings_google,\n",
    "        documents=chunks,\n",
    "        vectorstore_name=\"Vit_All_Google_Embeddings\"\n",
    "    )\n",
    "    print(\"vector_store_google:\", vector_store_google._collection.count(), \"chunks.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_store_google: 585 chunks.\n"
     ]
    }
   ],
   "source": [
    "vector_store_google = Chroma(\n",
    "    persist_directory = LOCAL_VECTOR_STORE_DIR.as_posix() + \"/Vit_All_Google_Embeddings\",\n",
    "    embedding_function=embeddings_google)\n",
    "print(\"vector_store_google:\",vector_store_google._collection.count(),\"chunks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_documents(docs,search_with_score=False):\n",
    "    \"\"\"helper function to print documents.\"\"\"\n",
    "    if search_with_score:\n",
    "        # used for similarity_search_with_score\n",
    "        print(\n",
    "            f\"\\n{'-' * 100}\\n\".join(\n",
    "                [f\"Document {i+1}:\\n\\n\" + doc[0].page_content +\"\\n\\nscore:\"+str(round(doc[-1],3))+\"\\n\" \n",
    "                 for i, doc in enumerate(docs)]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # used for similarity_search or max_marginal_relevance_search\n",
    "        print(\n",
    "            f\"\\n{'-' * 100}\\n\".join(\n",
    "                [f\"Document {i+1}:\\n\\n\" + doc.page_content \n",
    "                 for i, doc in enumerate(docs)]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "\n",
      "score:0.521\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "\n",
      "score:0.521\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "\n",
      "score:0.521\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "\n",
      "score:0.521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'What is Diffuse to choose?'\n",
    "docs_withScores = vector_store_google.similarity_search_with_score(query,k=4)\n",
    "\n",
    "print_documents(docs_withScores,search_with_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarty of document_0 to the query: 0.7394\n",
      "Similarty of document_1 to the query: 0.7394\n",
      "Similarty of document_2 to the query: 0.7394\n",
      "Similarty of document_3 to the query: 0.7394\n"
     ]
    }
   ],
   "source": [
    "# Here the scores are dot products. So a higher score is better.\n",
    "\n",
    "query_embeddings = embeddings_google.embed_query(query)\n",
    "docs_embeddings = embeddings_google.embed_documents(\n",
    "    [docs_withScores[i][0].page_content \n",
    "     for i in range(len(docs_withScores))\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in range(len(docs_embeddings)):\n",
    "    dot_product = round(np.dot(query_embeddings, docs_embeddings[i]),4)\n",
    "    print(f\"Similarty of document_{i} to the query: {dot_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Vectorstore_backed_retriever(vectorstore,search_type=\"similarity\",k=4,score_threshold=None):\n",
    "    \"\"\"create a vectorsore-backed retriever\n",
    "    Parameters: \n",
    "        search_type: Defines the type of search that the Retriever should perform.\n",
    "            Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\"\n",
    "        k: number of documents to return (Default: 4) \n",
    "        score_threshold: Minimum relevance threshold for similarity_score_threshold (default=None)\n",
    "    \"\"\"\n",
    "    search_kwargs={}\n",
    "    if k is not None:\n",
    "        search_kwargs['k'] = k\n",
    "    if score_threshold is not None:\n",
    "        search_kwargs['score_threshold'] = score_threshold\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search\n",
    "base_retriever_google = Vectorstore_backed_retriever(vector_store_google,\"similarity\",k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get relevant documents\n",
    "\n",
    "query = 'what is Diffuse to Choose?'\n",
    "relevant_docs = base_retriever_google.get_relevant_documents(query)\n",
    "\n",
    "print_documents(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter,LongContextReorder\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "def create_compression_retriever(embeddings, base_retriever, chunk_size=500, k=16, similarity_threshold=None):\n",
    "    \"\"\"Build a ContextualCompressionRetriever.\n",
    "    We wrap the the base_retriever (a vectorstore-backed retriever) into a ContextualCompressionRetriever.\n",
    "    The compressor here is a Document Compressor Pipeline, which splits documents\n",
    "    into smaller chunks, removes redundant documents, filters out the most relevant documents,\n",
    "    and reorder the documents so that the most relevant are at the top and bottom of the list.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings: OpenAIEmbeddings, GoogleGenerativeAIEmbeddings or HuggingFaceInferenceAPIEmbeddings.\n",
    "        base_retriever: a vectorstore-backed retriever.\n",
    "        chunk_size (int): Documents will be splitted into smaller chunks using a CharacterTextSplitter with a default chunk_size of 500. \n",
    "        k (int): top k relevant chunks to the query are filtered using the EmbeddingsFilter. default =16.\n",
    "        similarity_threshold : minimum relevance threshold used by the EmbeddingsFilter.. default =None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. splitting documents into smaller chunks\n",
    "    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0, separator=\". \")\n",
    "    \n",
    "    # 2. removing redundant documents\n",
    "    redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "\n",
    "    # 3. filtering based on relevance to the query    \n",
    "    relevant_filter = EmbeddingsFilter(embeddings=embeddings, k=k, similarity_threshold=similarity_threshold) # similarity_threshold and top K\n",
    "\n",
    "    # 4. Reorder the documents \n",
    "    \n",
    "    # Less relevant document will be at the middle of the list and more relevant elements at the beginning or end of the list.\n",
    "    # Reference: https://python.langchain.com/docs/modules/data_connection/retrievers/long_context_reorder\n",
    "    reordering = LongContextReorder()\n",
    "\n",
    "    # 5. Create compressor pipeline and retriever\n",
    "    \n",
    "    pipeline_compressor = DocumentCompressorPipeline(\n",
    "        transformers=[splitter, redundant_filter, relevant_filter, reordering]  \n",
    "    )\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline_compressor, \n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "\n",
    "    return compression_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n"
     ]
    }
   ],
   "source": [
    "compression_retriever_Google = create_compression_retriever(\n",
    "    embeddings=embeddings_google,\n",
    "    base_retriever=base_retriever_google,\n",
    "    k=16)\n",
    "\n",
    "query = 'what is Diffuse to Choose?'\n",
    "\n",
    "compressed_docs = compression_retriever_Google.get_relevant_documents(query)\n",
    "\n",
    "print_documents(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain_community.llms import Cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CohereRerank_retriever(\n",
    "    base_retriever, \n",
    "    cohere_api_key,cohere_model=\"rerank-multilingual-v2.0\", top_n=8\n",
    "):\n",
    "    \"\"\"Build a ContextualCompressionRetriever using Cohere Rerank endpoint to reorder the results based on relevance.\n",
    "    Parameters:\n",
    "       base_retriever: a Vectorstore-backed retriever\n",
    "       cohere_api_key: the Cohere API key\n",
    "       cohere_model: The Cohere model can be either 'rerank-english-v2.0' or 'rerank-multilingual-v2.0', with the latter being the default.\n",
    "       top_n: top n results returned by Cohere rerank, default = 8.\n",
    "    \"\"\"\n",
    "    \n",
    "    compressor = CohereRerank(\n",
    "        cohere_api_key=cohere_api_key, \n",
    "        model=cohere_model, \n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    retriever_Cohere = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "    return retriever_Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranking using Cohere. Vcetorstore using Google Embeddings:\n",
      "\n",
      "Document 1:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "on incorporating fine-grained features from the reference\n",
      "image directly into the latent feature maps of the main dif-\n",
      "fusion model, alongside with a perceptual loss to further\n",
      "preserve the reference item’s details. We conduct extensive\n",
      "testing on both in-house and publicly available datasets,\n",
      "and show that Diffuse to Choose is superior to existing zero-\n",
      "1arXiv:2401.13795v1  [cs.CV]  24 Jan 2024\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API token\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "retriever_Cohere_google = CohereRerank_retriever(\n",
    "    base_retriever=base_retriever_google, \n",
    "    cohere_api_key=cohere_api_key, cohere_model=\"rerank-multilingual-v2.0\",  \n",
    "    top_n=8\n",
    ")\n",
    "\n",
    "query = 'what is Diffuse to Choose?'\n",
    "\n",
    "docs_cohere = retriever_Cohere_google.get_relevant_documents(query)\n",
    "\n",
    "print(\"Reranking using Cohere. Vcetorstore using Google Embeddings:\\n\")\n",
    "print_documents(docs_cohere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_blocks(\n",
    "    create_vectorstore=True,# if True a Chroma vectorstore is created, else the Chroma vectorstore will be loaded\n",
    "    LLM_service=\"Google\",\n",
    "    vectorstore_name=\"Vit_All_google_Embeddings\",\n",
    "    chunk_size = 1600, chunk_overlap=200, # parameters of the RecursiveCharacterTextSplitter\n",
    "    retriever_type=\"Vectorstore_backed_retriever\",\n",
    "    base_retriever_search_type=\"similarity\", base_retriever_k=10, base_retriever_score_threshold=None,\n",
    "    compression_retriever_k=16,\n",
    "    cohere_api_key=cohere_api_key, cohere_model=\"rerank-multilingual-v2.0\", cohere_top_n=8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rertieval includes: document loaders, text splitter, vectorstore and retriever. \n",
    "    \n",
    "    Parameters: \n",
    "        create_vectorstore (boolean): If True, a new Chroma vectorstore will be created. Otherwise, an existing vectorstore will be loaded.\n",
    "        LLM_service: OpenAI, Google or HuggingFace.\n",
    "        vectorstore_name (str): the name of the vectorstore.\n",
    "        chunk_size and chunk_overlap: parameters of the RecursiveCharacterTextSplitter, default = (1600,200).\n",
    "        \n",
    "        retriever_type (str): in [Vectorstore_backed_retriever,Contextual_compression,Cohere_reranker]\n",
    "        \n",
    "        base_retriever_search_type: search_type in [\"similarity\", \"mmr\", \"similarity_score_threshold\"], default = similarity.\n",
    "        base_retriever_k: The most similar vectors to retrieve (default k = 10).  \n",
    "        base_retriever_score_threshold: score_threshold used by the base retriever, default = None.\n",
    "\n",
    "        compression_retriever_k: top k documents returned by the compression retriever, default=16\n",
    "        \n",
    "        cohere_api_key: Cohere API key\n",
    "        cohere_model (str): The Cohere model can be either 'rerank-english-v2.0' or 'rerank-multilingual-v2.0', with the latter being the default.\n",
    "        cohere_top_n: top n results returned by Cohere rerank, default = 8.\n",
    "   \n",
    "    Output:\n",
    "        retriever.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create new Vectorstore (Chroma index)\n",
    "        if create_vectorstore: \n",
    "            # 1. load documents\n",
    "            documents = langchain_document_loader(TMP_DIR)\n",
    "            \n",
    "            # 2. Text Splitter: split documents to chunks\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],    \n",
    "                chunk_size = chunk_size,\n",
    "                chunk_overlap= chunk_overlap\n",
    "            )\n",
    "            chunks = text_splitter.split_documents(documents=documents)\n",
    "            \n",
    "            # 3. Embeddings\n",
    "            embeddings = select_embeddings_model(LLM_service=LLM_service)\n",
    "        \n",
    "            # 4. Vectorsore: create Chroma index\n",
    "            vector_store = create_vectorstore(\n",
    "                embeddings=embeddings,\n",
    "                documents = chunks,\n",
    "                vectorstore_name=vectorstore_name,\n",
    "            )\n",
    "    \n",
    "        # 5. Load a Vectorstore (Chroma index)\n",
    "        else: \n",
    "            embeddings = select_embeddings_model(LLM_service=LLM_service)        \n",
    "            vector_store = Chroma(\n",
    "                persist_directory = LOCAL_VECTOR_STORE_DIR.as_posix() + \"/\" + vectorstore_name,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            \n",
    "        # 6. base retriever: Vector store-backed retriever \n",
    "        base_retriever = Vectorstore_backed_retriever(\n",
    "            vector_store,\n",
    "            search_type=base_retriever_search_type,\n",
    "            k=base_retriever_k,\n",
    "            score_threshold=base_retriever_score_threshold\n",
    "        )\n",
    "        retriever = None\n",
    "        if retriever_type==\"Vectorstore_backed_retriever\": \n",
    "            retriever = base_retriever\n",
    "    \n",
    "        # 7. Contextual Compression Retriever\n",
    "        if retriever_type==\"Contextual_compression\":    \n",
    "            retriever = create_compression_retriever(\n",
    "                embeddings=embeddings,\n",
    "                base_retriever=base_retriever,\n",
    "                k=compression_retriever_k,\n",
    "            )\n",
    "    \n",
    "        # 8. CohereRerank retriever\n",
    "        if retriever_type==\"Cohere_reranker\":\n",
    "            retriever = CohereRerank_retriever(\n",
    "                base_retriever=base_retriever, \n",
    "                cohere_api_key=cohere_api_key, \n",
    "                cohere_model=cohere_model, \n",
    "                top_n=cohere_top_n\n",
    "            )\n",
    "    \n",
    "        print(f\"\\n{retriever_type} is created successfully!\")\n",
    "        print(f\"Relevant documents will be retrieved from vectorstore ({vectorstore_name}) which uses {LLM_service} embeddings \\\n",
    "and has {vector_store._collection.count()} chunks.\")\n",
    "        \n",
    "        return retriever\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_LLM(LLM_provider,api_key,temperature=0.5,top_p=0.95,model_name=None):\n",
    "    \"\"\"Instantiate LLM in Langchain.\n",
    "    Parameters:\n",
    "        LLM_provider (str): the LLM provider; in [\"OpenAI\",\"Google\",\"HuggingFace\"]\n",
    "        model_name (str): in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0125\", \"gpt-4-turbo-preview\", \n",
    "            \"gemini-pro\", \"mistralai/Mistral-7B-Instruct-v0.2\"].            \n",
    "        api_key (str): google_api_key or openai_api_key or huggingfacehub_api_token \n",
    "        temperature (float): Range: 0.0 - 1.0; default = 0.5\n",
    "        top_p (float): : Range: 0.0 - 1.0; default = 1.\n",
    "    \"\"\"\n",
    "    if LLM_provider == \"OpenAI\":\n",
    "        llm = ChatOpenAI(\n",
    "            api_key=api_key,\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            model_kwargs={\n",
    "                \"top_p\": top_p\n",
    "            }\n",
    "        )\n",
    "    if LLM_provider == \"Google\":\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            google_api_key=api_key,\n",
    "            # model=\"gemini-pro\",\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "    if LLM_provider == \"HuggingFace\":\n",
    "        llm = HuggingFaceHub(\n",
    "            repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "            #repo_id=model_name,\n",
    "            huggingfacehub_api_token=api_key,\n",
    "            model_kwargs={\n",
    "                \"temperature\":temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\":1024\n",
    "            },\n",
    "        )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory,ConversationBufferMemory\n",
    "\n",
    "def create_memory(model_name='gpt-3.5-turbo',memory_max_token=None):\n",
    "    \"\"\"Creates a ConversationSummaryBufferMemory for gpt-3.5-turbo\n",
    "    Creates a ConversationBufferMemory for the other models.\"\"\"\n",
    "    \n",
    "    if model_name==\"gpt-3.5-turbo\":\n",
    "        if memory_max_token is None:\n",
    "            memory_max_token = 1024 # max_tokens for 'gpt-3.5-turbo' = 4096\n",
    "        memory = ConversationSummaryBufferMemory(\n",
    "            max_token_limit=memory_max_token,\n",
    "            llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\",openai_api_key=openai_api_key,temperature=0.1),\n",
    "            return_messages=True,\n",
    "            memory_key='chat_history',\n",
    "            output_key=\"answer\",\n",
    "            input_key=\"question\"\n",
    "        )\n",
    "    else:\n",
    "        memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key='chat_history',\n",
    "            output_key=\"answer\",\n",
    "            input_key=\"question\",\n",
    "        )  \n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='what does DTC stand for?'),\n",
       "  AIMessage(content='Diffuse to Choose (DTC) is a novel diffusion inpainting approach designed for the Vit-All application, \\n    which allows users to virtually place any e-commerce item in any setting, ensuring detailed, semantically coherent blending with realistic \\n    lighting and shadows. It effectively incorporates fine-grained cues from the reference image into the main U-Net decoder \\n    using a secondary U-Net encoder.\\n    DTC can handle a variety of e-commerce products and can generate images using in-the-wild images & references. \\n    It is superior to existing zero-shot personalization methods, especially in preserving the fine-grained details of items.'),\n",
       "  HumanMessage(content='what does Vit-all stand for?'),\n",
       "  AIMessage(content='Virtual Try-All')]}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = create_memory(model_name='gemini-pro',memory_max_token=20)\n",
    "\n",
    "# save context\n",
    "memory.save_context(\n",
    "    inputs={\"question\":\"what does DTC stand for?\"},\n",
    "    outputs={\"answer\":\"\"\"Diffuse to Choose (DTC) is a novel diffusion inpainting approach designed for the Vit-All application, \n",
    "    which allows users to virtually place any e-commerce item in any setting, ensuring detailed, semantically coherent blending with realistic \n",
    "    lighting and shadows. It effectively incorporates fine-grained cues from the reference image into the main U-Net decoder \n",
    "    using a secondary U-Net encoder.\n",
    "    DTC can handle a variety of e-commerce products and can generate images using in-the-wild images & references. \n",
    "    It is superior to existing zero-shot personalization methods, especially in preserving the fine-grained details of items.\"\"\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"question\":\"what does Vit-all stand for?\"},\n",
    "    outputs={\"answer\":\"Virtual Try-All\"}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "standalone_question_template = \"\"\"Given the following conversation and a follow up question, \n",
    "rephrase the follow up question to be a standalone question, in its original language.\\n\\n\n",
    "Chat History:\\n{chat_history}\\n\n",
    "Follow Up Input: {question}\\n\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "standalone_question_prompt = PromptTemplate(\n",
    "    input_variables=['chat_history', 'question'], \n",
    "    template=standalone_question_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Given the following conversation and a follow up question, \\nrephrase the follow up question to be a standalone question, in its original language.\\n\\n\\nChat History:\\nHuman: what does DTC stand for?\\nAI: DTC stands for Diffuse to choose.\\n\\nFollow Up Input: plaese give more details about it, including its use cases and implementation.\\n\\nStandalone question:')"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory = create_memory(model_name='gemini-pro')\n",
    "memory.save_context(\n",
    "    inputs={\"question\":\"what does DTC stand for?\"}, \n",
    "    outputs={\"answer\":\"DTC stands for Diffuse to choose.\"}\n",
    ")\n",
    "\n",
    "standalone_question_prompt.invoke(\n",
    "    {\"question\":\"plaese give more details about it, including its use cases and implementation.\",\n",
    "    \"chat_history\":memory.chat_memory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_template(language=\"english\"):\n",
    "    \"\"\"Pass the standalone question along with the chat history and context \n",
    "    to the `LLM` wihch will answer\"\"\"\n",
    "    \n",
    "    template = f\"\"\"Answer the question at the end, using only the following context (delimited by <context></context>).\n",
    "Your answer must be in the language at the end. \n",
    "\n",
    "<context>\n",
    "{{chat_history}}\n",
    "\n",
    "{{context}} \n",
    "</context>\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Language: {language}.\n",
    "\"\"\"\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"Answer the question at the end, using only the following context (delimited by <context></context>).\\nYour answer must be in the language at the end. \\n\\n<context>\\nHuman: what does DTC stand for?\\nAI: DTC stands for Diffuse to choose.\\n\\n[Document(page_content='DTC use cases include...')] \\n</context>\\n\\nQuestion: plaese give more details about DTC, including its use cases and implementation.\\n\\nLanguage: english.\\n\")])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "answer_prompt = ChatPromptTemplate.from_template(answer_template())\n",
    "\n",
    "# invoke the ChatPromptTemplate\n",
    "answer_prompt.invoke(\n",
    "    {\"question\":\"plaese give more details about DTC, including its use cases and implementation.\",\n",
    "     \"context\":[Document(page_content=\"DTC use cases include...\")], # the context is a list of retrieved documents.\n",
    "     \"chat_history\":memory.chat_memory}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example using gemini-pro\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API token\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if the token is loaded correctly\n",
    "\n",
    "# Create a Conversational Retrieval Chain using Mistral-7B-Instruct-v0.2\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    condense_question_prompt=standalone_question_prompt,\n",
    "    combine_docs_chain_kwargs={'prompt': answer_prompt},\n",
    "    condense_question_llm=instantiate_LLM(\n",
    "        LLM_provider=\"Google\",api_key=google_api_key,temperature=0.1,\n",
    "        model_name=\"gemini-pro\"\n",
    "    ),\n",
    "\n",
    "    memory=create_memory(\"gemini-pro\"),\n",
    "    retriever = base_retriever_google, \n",
    "    llm=instantiate_LLM(\n",
    "        LLM_provider=\"Google\",api_key=google_api_key,temperature=0.5,\n",
    "        model_name=\"gemini-pro\"),\n",
    "    chain_type= \"stuff\",\n",
    "    verbose= False,\n",
    "    return_source_documents=True   \n",
    ")\n",
    "\n",
    "# Example note: If using Gemini-Pro or other models, modify model_name accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is DTC?',\n",
       " 'chat_history': [HumanMessage(content='what does DTC stand for?'),\n",
       "  AIMessage(content='The provided context does not mention what DTC stands for, so I cannot answer this question from the provided context.'),\n",
       "  HumanMessage(content='plaese give more details about it, including its use cases and implementation.'),\n",
       "  AIMessage(content='The provided context does not mention what DTC stands for or its use cases and implementation, so I cannot answer this question from the provided context.'),\n",
       "  HumanMessage(content='what is DTC?'),\n",
       "  AIMessage(content='The provided context does not mention what DTC stands for, so I cannot answer this question from the provided context.')],\n",
       " 'answer': 'The provided context does not mention what DTC stands for, so I cannot answer this question from the provided context.',\n",
       " 'source_documents': [Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'}),\n",
       "  Document(page_content='Figure 5. DTC can handle variety of e-commerce products and can generate images using in-the-wild images & references.\\n6', metadata={'page': 5, 'source': 'C:\\\\d drive\\\\Anurag\\\\MAchine Learning\\\\Projects\\\\Rag_langchain_chatbot\\\\data\\\\tmp\\\\2024-01-24-VirtualTryAll.pdf'})]}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's invoke the chain\n",
    "response = chain.invoke({\"question\":\"what is DTC?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='what does DTC stand for?'),\n",
       "  AIMessage(content='The provided context does not mention what DTC stands for, so I cannot answer this question from the provided context.'),\n",
       "  HumanMessage(content='plaese give more details about it, including its use cases and implementation.'),\n",
       "  AIMessage(content='The provided context does not mention what DTC stands for or its use cases and implementation, so I cannot answer this question from the provided context.'),\n",
       "  HumanMessage(content='what is DTC?'),\n",
       "  AIMessage(content='The provided context does not mention what DTC stands for, so I cannot answer this question from the provided context.')]}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get chat history of the chain's memory\n",
    "\n",
    "chain.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided context does not mention what DTC stands for or its use cases and implementation, so I cannot answer this question from the provided context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "follow_up_question = \"plaese give more details about it, including its use cases and implementation.\"\n",
    "\n",
    "Markdown(chain.invoke({\"question\":follow_up_question})['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "def create_ConversationalRetrievalChain(\n",
    "    llm,condense_question_llm,\n",
    "    retriever,\n",
    "    chain_type= 'stuff',\n",
    "    language=\"english\",\n",
    "    model_name='gpt-3.5-turbo'\n",
    "):\n",
    "    \"\"\"Create a ConversationalRetrievalChain.\n",
    "    First, it passes the follow-up question along with the chat history to an LLM which rephrases \n",
    "    the question and generates a standalone query. \n",
    "    This query is then sent to the retriever, which fetches relevant documents (context) \n",
    "    and passes them along with the standalone question and chat history to an LLM to answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define the standalone_question prompt. \n",
    "    # Pass the follow-up question along with the chat history to the `condense_question_llm`\n",
    "    # which rephrases the question and generates a standalone question.\n",
    "\n",
    "    standalone_question_prompt = PromptTemplate(\n",
    "        input_variables=['chat_history', 'question'], \n",
    "        template=\"\"\"Given the following conversation and a follow up question, \n",
    "rephrase the follow up question to be a standalone question, in its original language.\\n\\n\n",
    "Chat History:\\n{chat_history}\\n\n",
    "Follow Up Input: {question}\\n\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "    # 2. Define the answer_prompt\n",
    "    # Pass the standalone question + the chat history + the context (retrieved documents) to the `LLM` wihch will answer\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(answer_template(language=language))\n",
    "\n",
    "    # 3. Add ConversationSummaryBufferMemory for gpt-3.5, and ConversationBufferMemory for the other models\n",
    "    \n",
    "    memory = create_memory(model_name)\n",
    "\n",
    "    # 4. Create the ConversationalRetrievalChain\n",
    "\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        condense_question_prompt=standalone_question_prompt,\n",
    "        combine_docs_chain_kwargs={'prompt': answer_prompt},\n",
    "        condense_question_llm=condense_question_llm,\n",
    "\n",
    "        memory=memory,\n",
    "        retriever = retriever,\n",
    "        llm=llm,\n",
    "\n",
    "        chain_type= chain_type,\n",
    "        verbose= False,\n",
    "        return_source_documents=True    \n",
    "    )\n",
    "\n",
    "    print(\"Conversational retriever chain created successfully!\")\n",
    "    \n",
    "    return chain,memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import format_document,Document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use a simple `ConversationBufferMemory' here for simplicity.\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",output_key=\"answer\", input_key=\"question\",return_messages=True)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': []}\n",
      "{'x': 1, 'chat_history': []}\n"
     ]
    }
   ],
   "source": [
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    ")\n",
    "print(loaded_memory.invoke({}))\n",
    "print(loaded_memory.invoke({\"x\":1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pass the follow-up question along with the chat history to the LLM, and parse the answer (standalone_question).\n",
    "\n",
    "condense_question_prompt = PromptTemplate(\n",
    "    input_variables=['chat_history', 'question'], \n",
    "    template=\"\"\"Given the following conversation and a follow up question (at the end), \n",
    "rephrase the follow up question to be a standalone question, in the same language as the follow up question.\\n\\n\n",
    "Chat History:\\n{chat_history}\\n\n",
    "Follow up question: {question}\\n\n",
    "Standalone question:\"\"\"\n",
    ")\n",
    "\n",
    "condense_question_llm = instantiate_LLM(\n",
    "    LLM_provider=\"Google\",api_key=google_api_key,temperature=0.1,\n",
    "    model_name=\"gemini-pro\"\n",
    ")\n",
    "\n",
    "standalone_question_chain = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | condense_question_prompt\n",
    "    | condense_question_llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "# 3. Combine load_memory and standalone_question_chain\n",
    "chain_question = loaded_memory | standalone_question_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history:\n",
      " {'chat_history': [HumanMessage(content='What does DTC stand for?'), AIMessage(content='Diffuse to Choose.')]}\n",
      "\n",
      "Follow-up question:\n",
      " plaese give more details about it, including its use cases and implementation.\n",
      "\n",
      "Standalone_question:\n",
      " Can you provide more information about Diffuse to Choose (DTC), including its use cases and implementation?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here is an example of how to invoke chain_question:\n",
    "\n",
    "memory.clear()\n",
    "memory.save_context(\n",
    "    {\"question\": \"What does DTC stand for?\"},\n",
    "    {\"answer\": \"Diffuse to Choose.\"}\n",
    ")\n",
    "\n",
    "print(\"Chat history:\\n\",memory.load_memory_variables({}))\n",
    "print(\"\\nFollow-up question:\\n\",\"plaese give more details about it, including its use cases and implementation.\")\n",
    "\n",
    "# invoke chain_question\n",
    "response = chain_question.invoke({\"question\":\"plaese give more details about it, including its use cases and implementation.\"})['standalone_question']\n",
    "print(\"\\nStandalone_question:\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cohere_reranker is created successfully!\n",
      "Relevant documents will be retrieved from vectorstore (Vit_All_Google_Embeddings) which uses Google embeddings and has 585 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Create retriever \n",
    "retriever_Google = retrieval_blocks(\n",
    "    create_vectorstore=False,\n",
    "    LLM_service=\"Google\",\n",
    "    vectorstore_name=\"Vit_All_Google_Embeddings\",\n",
    "    retriever_type=\"Cohere_reranker\",\n",
    "    base_retriever_search_type=\"similarity\", base_retriever_k=12,\n",
    "    compression_retriever_k=16,\n",
    "    cohere_api_key=cohere_api_key,cohere_top_n=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def _combine_documents(docs, document_prompt, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever_Google,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "# Get variables ['chat_history', 'context', 'question'] that will be passed to `answer_prompt`\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "answer_prompt = ChatPromptTemplate.from_template(answer_template()) # 3 variables are expected ['chat_history', 'context', 'question']\n",
    "\n",
    "answer_prompt_variables = {\n",
    "    \"context\": lambda x: _combine_documents(docs=x[\"docs\"],document_prompt=DEFAULT_DOCUMENT_PROMPT),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"chat_history\": itemgetter(\"chat_history\") # get chat_history from `loaded_memory` variable\n",
    "}\n",
    "\n",
    "# Load memory, format `answer_prompt` with variables (context, question and chat_history) and pass the `answer_prompt to LLM.\n",
    "# return answer, docs and standalone_question\n",
    "\n",
    "llm = instantiate_LLM(\n",
    "    LLM_provider=\"Google\",api_key=google_api_key,temperature=0.5,\n",
    "    model_name=\"gemini-pro\"\n",
    ")\n",
    "\n",
    "chain_answer = {\n",
    "    \"answer\": loaded_memory | answer_prompt_variables | answer_prompt | llm,\n",
    "    \"docs\": lambda x: [\n",
    "        Document(page_content=doc.page_content,metadata=doc.metadata) # return only page_content and metadata (_DocumentWithState returns them + state.)\n",
    "        for doc in x[\"docs\"]\n",
    "    ], \n",
    "    \"standalone_question\": lambda x:x[\"question\"] # return standalone_question\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversational_retriever_chain = chain_question | retrieved_documents | chain_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='What does DTC stand for?'),\n",
       "  AIMessage(content='Diffuse to Choose.')]}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided context does not contain any information about the use cases or implementation of Diffuse to Choose (DTC)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "follow_up_question = \"plaese give more details about it, including its use cases and implementation.\"\n",
    "\n",
    "response = conversational_retriever_chain.invoke({\"question\":follow_up_question})\n",
    "Markdown(response['answer'].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='What does DTC stand for?'),\n",
       "  AIMessage(content='Diffuse to Choose.')]}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='What does DTC stand for?'),\n",
       "  AIMessage(content='Diffuse to Choose.'),\n",
       "  HumanMessage(content='plaese give more details about it, including its use cases and implementation.'),\n",
       "  AIMessage(content='The provided context does not contain any information about the use cases or implementation of Diffuse to Choose (DTC).')]}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory.save_context( {\"question\": follow_up_question}, {\"answer\": response['answer'].content} ) # update memory\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_template(language=\"english\"):\n",
    "    \"\"\"Pass the standalone question along with the chat history and context (retrieved documents) to the `LLM` to get an answer.\"\"\"\n",
    "    \n",
    "    template = f\"\"\"Answer the question at the end, using only the following context (delimited by <context></context>).\n",
    "Your answer must be in the language at the end. \n",
    "\n",
    "<context>\n",
    "{{chat_history}}\n",
    "\n",
    "{{context}} \n",
    "</context>\n",
    "\n",
    "Question: {{question}}\n",
    "\n",
    "Language: {language}.\n",
    "\"\"\"\n",
    "    return template\n",
    "    \n",
    "def _combine_documents(docs, document_prompt, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "def custom_ConversationalRetrievalChain(\n",
    "    llm,condense_question_llm,\n",
    "    retriever,\n",
    "    language=\"english\",\n",
    "    llm_provider=\"OpenAI\",\n",
    "    model_name='gpt-3.5-turbo',\n",
    "):\n",
    "    \"\"\"Create a ConversationalRetrievalChain step by step.\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Step 1: Create a standalone_question chain\n",
    "    ##############################################################\n",
    "    \n",
    "    # 1. Create memory: ConversationSummaryBufferMemory for gpt-3.5, and ConversationBufferMemory for the other models\n",
    "    \n",
    "    memory = create_memory(model_name)\n",
    "    # memory = ConversationBufferMemory(memory_key=\"chat_history\",output_key=\"answer\", input_key=\"question\",return_messages=True)\n",
    "\n",
    "    # 2. load memory using RunnableLambda. Retrieves the chat_history attribute using itemgetter.\n",
    "    loaded_memory = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    "    )\n",
    "\n",
    "    # 3. Pass the follow-up question along with the chat history to the LLM, and parse the answer (standalone_question).\n",
    "\n",
    "    condense_question_prompt = PromptTemplate(\n",
    "        input_variables=['chat_history', 'question'], \n",
    "        template = \"\"\"Given the following conversation and a follow up question, \n",
    "rephrase the follow up question to be a standalone question, in the same language as the follow up question.\\n\\n\n",
    "Chat History:\\n{chat_history}\\n\n",
    "Follow Up Input: {question}\\n\n",
    "Standalone question:\"\"\"        \n",
    ")\n",
    "        \n",
    "    standalone_question_chain = {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "        }\n",
    "        | condense_question_prompt\n",
    "        | condense_question_llm\n",
    "        | StrOutputParser(),\n",
    "    }\n",
    "    \n",
    "    # 4. Combine load_memory and standalone_question_chain\n",
    "    chain_question = loaded_memory | standalone_question_chain\n",
    "    \n",
    "    ####################################################################################\n",
    "    #   Step 2: Retrieve documents, pass them to the LLM, and return the response.\n",
    "    ####################################################################################\n",
    "\n",
    "    # 5. Retrieve relevant documents\n",
    "    retrieved_documents = {\n",
    "        \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    \n",
    "    # 6. Get variables ['chat_history', 'context', 'question'] that will be passed to `answer_prompt`\n",
    "    \n",
    "    DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "    answer_prompt = ChatPromptTemplate.from_template(answer_template(language=language)) \n",
    "    # 3 variables are expected ['chat_history', 'context', 'question'] by the ChatPromptTemplate   \n",
    "    answer_prompt_variables = {\n",
    "        \"context\": lambda x: _combine_documents(docs=x[\"docs\"],document_prompt=DEFAULT_DOCUMENT_PROMPT),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\") # get it from `loaded_memory` variable\n",
    "    }\n",
    "    \n",
    "    # 7. Load memory, format `answer_prompt` with variables (context, question and chat_history) and pass the `answer_prompt to LLM.\n",
    "    # return answer, docs and standalone_question\n",
    "    \n",
    "    chain_answer = {\n",
    "        \"answer\": loaded_memory | answer_prompt_variables | answer_prompt | llm,\n",
    "        # return only page_content and metadata \n",
    "        \"docs\": lambda x: [Document(page_content=doc.page_content,metadata=doc.metadata) for doc in x[\"docs\"]],\n",
    "        \"standalone_question\": lambda x:x[\"question\"] # return standalone_question\n",
    "    }\n",
    "\n",
    "    # 8. Final chain\n",
    "    conversational_retriever_chain = chain_question | retrieved_documents | chain_answer\n",
    "\n",
    "    print(\"Conversational retriever chain created successfully!\")\n",
    "\n",
    "    return conversational_retriever_chain,memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "\n",
    "# QUESTIONS\n",
    "questions = [\"what does DTC stands for?\",\n",
    "             \"plaese give more details about it, including its use cases and implementation.\",\n",
    "             \"does it outperform other diffusion-based models? explain in details.\",\n",
    "             \"what is Langchain?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cohere_reranker is created successfully!\n",
      "Relevant documents will be retrieved from vectorstore (Vit_All_Google_Embeddings) which uses Google embeddings and has 585 chunks.\n",
      "Conversational retriever chain created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the retriever and the ConversationalRetrievalChain :\n",
    "\n",
    "retriever_Google = retrieval_blocks(\n",
    "    create_vectorstore=False,\n",
    "    LLM_service=\"Google\",\n",
    "    vectorstore_name=\"Vit_All_Google_Embeddings\",\n",
    "    retriever_type=\"Cohere_reranker\",\n",
    "    base_retriever_search_type=\"similarity\", base_retriever_k=12,\n",
    "    compression_retriever_k=16,\n",
    "    cohere_api_key=cohere_api_key,cohere_top_n=10,\n",
    ")\n",
    "\n",
    "chain_gemini,memory_gemini = custom_ConversationalRetrievalChain(\n",
    "    llm = instantiate_LLM(\n",
    "        LLM_provider=\"Google\",api_key=google_api_key,temperature=0.5,model_name=\"gemini-pro\"\n",
    "    ),\n",
    "    condense_question_llm = instantiate_LLM(\n",
    "        LLM_provider=\"Google\",api_key=google_api_key,temperature=0.1,model_name=\"gemini-pro\"),\n",
    "    retriever=retriever_Google,\n",
    "    language=\"english\",\n",
    "    llm_provider=\"Google\",\n",
    "    model_name=\"gemini-pro\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question[0]: what does DTC stands for?\n",
      "Standalone_question: What is the meaning of DTC?\n",
      "Answer:\n",
      " The provided context does not mention the meaning of DTC, so I cannot answer this question from the provided context. \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Question[1]: plaese give more details about it, including its use cases and implementation.\n",
      "Standalone_question: What are the details of DTC, including its use cases and implementation?\n",
      "Answer:\n",
      " The provided context does not mention the details of DTC, including its use cases and implementation, so I cannot answer this question from the provided context. \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Question[2]: does it outperform other diffusion-based models? explain in details.\n",
      "Standalone_question: How does DTC compare to other diffusion-based models in terms of performance?\n",
      "Answer:\n",
      " The provided context does not mention how DTC compares to other diffusion-based models in terms of performance, so I cannot answer this question from the provided context. \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Question[3]: what is Langchain?\n",
      "Standalone_question: What is Langchain?\n",
      "Answer:\n",
      " The provided context does not mention Langchain, so I cannot answer this question from the provided context. \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "CPU times: total: 562 ms\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "memory_gemini.clear()\n",
    "\n",
    "for i,question in enumerate(questions):\n",
    "    response = chain_gemini.invoke({\"question\":question})\n",
    "    answer = response['answer'].content\n",
    "    print(f\"Question[{i}]:\",question)\n",
    "    print(\"Standalone_question:\",response['standalone_question'])\n",
    "    print(\"Answer:\\n\",answer,f\"\\n\\n{'-' * 100}\\n\")\n",
    "    \n",
    "    memory_gemini.save_context( {\"question\": question}, {\"answer\": answer} ) # update memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
